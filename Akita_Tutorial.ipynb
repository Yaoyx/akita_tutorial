{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTjc_uNPZgr8"
      },
      "source": [
        "# Deep Learning in 3D Genome (Tutorial)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2wKpUC-3Uh1"
      },
      "source": [
        "## Explore model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BgVlS0p3D-P"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import subprocess\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '-1' ### run on CPU\n",
        "\n",
        "from cooltools.lib.numutils import set_diag\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pysam\n",
        "import tensorflow as tf\n",
        "from basenji import dataset, dna_io, seqnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdneYEDIzFTl"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbHq11SqRxPw"
      },
      "source": [
        "### Understanding the Model Architecture\n",
        "Akita (and other basenji-derived models) stores the information needed to specify its architecture in a json file. Achitectures are specified in terms of blocks and layers, where each block can have multiple layers.\n",
        "\n",
        "The weights for Akita, which were learned via training, are stored separately in a standard hdf5 file generated by tensorflow.\n",
        "\n",
        "Before diving into the Keras model summary, let's examine the structure/architecture of a model and understand how the shapes of tensors change throughout the model. Below we print the part of the json file that specifies parameters of the first few blocks.\n",
        "\n",
        "Note the first block is a convolution followed by pooling. The second block is a tower of repeated convolutions followed by pooling. Together this pooling takes us from an input sequence of 1310720 to a set of latent profiles binned at $2^{11}$ bp resolution. This is also the resolution of the target maps, as no more pooling is performed in this network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77VxV531HhCZ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Open and read the JSON file\n",
        "model_dir = './tutorial_materials/akita_v2/'\n",
        "params_file = model_dir+'params.json' # architecture\n",
        "with open(params_file, 'r') as file:\n",
        "    params = json.load(file)\n",
        "    model_architecture = params['model'] # Retrieve model's architecture from params.json\n",
        "\n",
        "model_architecture['trunk'][:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iGI9xzUSJXC"
      },
      "source": [
        "Below we initialize the model architecture and print the parital keras summary of the first seven layers.\n",
        "\n",
        "These incude: \\\n",
        "1. **Input Layer**: The input layer receives sequences in a specific shape (e.g., `(batch_size, sequence_length, channels)`).\n",
        "2. **First Convolutional Layer**: Applies convolution with specified filters and kernel size, resulting in shape transformation.\n",
        "3. **Pooling Layer**: Reduces the spatial dimensions.\n",
        "4. **ReLu Layer**: Activation layer, providing non-linearity to the network; returns zero for negative inputs, and the same value for positive inputs.\n",
        "\n",
        "Note that the first dimension is always None, as models can process a flexibile number of sequences (determined by the batch size) both when training or making predictions. See [tensorflow docs](https://www.tensorflow.org/js/guide/models_and_layers#model_summary) for more information.\n",
        "\n",
        "The stochastic shift and stochastic reverse compliment layers are used by Akita (and Basenji) for data augmentation during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbRXVMaW3D-S",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from akita_utils import print_partial_model_summary\n",
        "\n",
        "human_model = seqnn.SeqNN(model_architecture)\n",
        "print_partial_model_summary(human_model.model, num_layers=7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTuyRZuzSoPJ"
      },
      "source": [
        "- ðŸŒŸ Once we initialize the architecture of Akita model, we will need to restore the pretrained model weights. Since weights correspond to specific layers, successfully restoring weights requires that they are used with the same architecture as used when the model was originally trained. Here we use the architecture specified in the params.json file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6X8UqC7D3D-T"
      },
      "outputs": [],
      "source": [
        "head = 0 # index 0 indicates human model\n",
        "weights_file  = model_dir+f'model{head}_best.h5' # model_weights\n",
        "human_model.restore(weights_file)\n",
        "print('successfully loaded')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5aZiw6F3D-W"
      },
      "source": [
        "### Make a prediction from sequence (for human)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3ydbT-TC9D_"
      },
      "source": [
        "- ðŸŒŸ A FASTA file is a text-based format for representing nucleotide or peptide sequences. Each sequence in a FASTA file is preceded by a single-line description starting with a `>` character, followed by lines of sequence data. \\\n",
        "\\\n",
        "Example:\n",
        "\n",
        "> \\>sequence1 \\\n",
        "ATGCGTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTA \\\n",
        "\\>sequence2 \\\n",
        "CGTAGCTAGCTAGCTAGCTAACGATCGTAGCTAGCTAGCTAGCTAGCTAGCTAGC\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HqDNuIWY-t0"
      },
      "source": [
        "- ðŸŒŸ We use pysam to read sequence from fasta file, and encode the four nucleotides into one-hot encoding format, which is a binary matrix. \\\n",
        "\n",
        "<center>\n",
        "\n",
        "| Nucleotide | One-Hot Encoding  |\n",
        "|------------|-------------------|\n",
        "| A          | [1, 0, 0, 0]      |\n",
        "| C          | [0, 1, 0, 0]      |\n",
        "| G          | [0, 0, 1, 0]      |\n",
        "| T          | [0, 0, 0, 1]      |\n",
        "<center/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q55ljZ-r3D-X"
      },
      "outputs": [],
      "source": [
        "fasta_file = pysam.FastaFile('/content/akita_tutorial/tutorial_materials/data/chr11_99928064_101238784.fasta')\n",
        "seq = fasta_file.fetch(fasta_file.references[0])\n",
        "seq_1hot = dna_io.dna_1hot(seq)\n",
        "\n",
        "# expand input dimensions, as model accepts arrays of size [#regions,2^20bp, 4]\n",
        "test_pred_from_seq = human_model.model.predict(np.expand_dims(seq_1hot,0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ON1P7QvWZ5sd"
      },
      "source": [
        "- ðŸŒŸ The last step is plotting the HiC map! Since the model has multiple celltype outputs, for plotting an example, we just visualize the first cell type, which is indicates by `target_index` variable \\\n",
        "\\\n",
        "Note: Since model outputs are flattened to be upper triangular, additional information on the amount of cropping and diagonal offset is required to convert this output to a square matrix used for visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9b05Vmft7gxn"
      },
      "outputs": [],
      "source": [
        "# read data parameters\n",
        "data_stats_file = './tutorial_materials/akita_v2/data_params.json'\n",
        "with open(data_stats_file) as data_stats_open:\n",
        "    data_stats = json.load(data_stats_open)\n",
        "seq_length = data_stats['seq_length']\n",
        "target_length = data_stats['target_length']\n",
        "hic_diags =  data_stats['diagonal_offset']\n",
        "target_crop = data_stats['crop_bp'] // data_stats['pool_width']\n",
        "target_length1 = data_stats['seq_length'] // data_stats['pool_width']\n",
        "\n",
        "target_length1_cropped = target_length1 - 2*target_crop\n",
        "def from_upper_triu(vector_repr, matrix_len, num_diags):\n",
        "    z = np.zeros((matrix_len,matrix_len))\n",
        "    triu_tup = np.triu_indices(matrix_len,num_diags)\n",
        "    z[triu_tup] = vector_repr\n",
        "    for i in range(-num_diags+1,num_diags):\n",
        "        set_diag(z, np.nan, i)\n",
        "    return z + z.T\n",
        "\n",
        "#transform from flattened representation to symmetric matrix representation\n",
        "target_index = 0\n",
        "mat = from_upper_triu(test_pred_from_seq[:,:,target_index], target_length1_cropped, hic_diags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JY04IX2Iwcp7"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,4))\n",
        "vmin=-2; vmax=2\n",
        "\n",
        "plt.subplot(121)\n",
        "im = plt.matshow(mat, fignum=False, cmap= 'RdBu_r', vmax=vmax, vmin=vmin)\n",
        "plt.colorbar(im, fraction=.04, pad = 0.05, ticks=[-2,-1, 0, 1,2]);\n",
        "plt.title('Akita prediction',y=1.15 )\n",
        "plt.ylabel('chr11:99928064-101238784')\n",
        "\n",
        "# plot target\n",
        "test_target = np.load('/content/akita_tutorial/tutorial_materials/data/chr11_99928064_101238784_target.npy')\n",
        "plt.subplot(122)\n",
        "mat = from_upper_triu(test_target[:,:,target_index], target_length1_cropped, hic_diags)\n",
        "im = plt.matshow(mat, fignum=False, cmap= 'RdBu_r', vmax=vmax, vmin=vmin)\n",
        "plt.colorbar(im, fraction=.04, pad = 0.05, ticks=[-2,-1, 0, 1,2]);\n",
        "plt.title('target',y=1.15)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci4UZhojExrw"
      },
      "source": [
        "### Predictions for distrupted genomic sequences (for mouse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iepY5E-qSVdc"
      },
      "source": [
        "- ðŸŒŸ Next, let's delve into more uses of the Akita model. With Akita, we can simulate the effects of sequence perturbations on genomic 3D organization. To start, we'll first download the reference genome of the mouse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f09zZ4bG-X1"
      },
      "outputs": [],
      "source": [
        "if not os.path.isfile('./tutorial_materials/data/mm10.ml.fa'):\n",
        "    print('downloading mm10.ml.fa')\n",
        "    subprocess.call('curl -o ./tutorial_materials/data/mm10.ml.fa.gz ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M22/GRCm38.p6.genome.fa.gz', shell=True)\n",
        "    subprocess.call('gunzip ./tutorial_materials/data/mm10.ml.fa.gz', shell=True)\n",
        "\n",
        "fasta_open = pysam.Fastafile('./tutorial_materials/data/mm10.ml.fa')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvAAfu3IVvEG"
      },
      "source": [
        "- ðŸŒŸ Load a table that stores genomic locations that contain CTCF motifs and a table records chromosome sizes of mouse genome"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTtCTYkhKx-e"
      },
      "outputs": [],
      "source": [
        "# model arguments\n",
        "CTCT_table = \"./tutorial_materials/data/insertion_disruption_tsvs/disruption_examples.tsv\"\n",
        "chrom_sizes = \"./tutorial_materials/data/mm10.fa.sizes\"\n",
        "chrom_sizes_table = pd.read_csv(chrom_sizes, sep=\"\\t\", names=[\"chrom\", \"size\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijLwCSCBa_ll"
      },
      "source": [
        "- ðŸŒŸ This time we will load the pretrained weights for mouse instead of human, but the model architecture is same for both of mouse and human model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwZQH1ysMI5A"
      },
      "outputs": [],
      "source": [
        "head = 1 # index 1 indicates mouse model\n",
        "shifts = \"0\"\n",
        "shifts = [int(shift) for shift in shifts.split(\",\")]\n",
        "rc = False\n",
        "weights_file  = model_dir+f'model{head}_best.h5' # model_weights\n",
        "\n",
        "mouse_model = seqnn.SeqNN(model_architecture)\n",
        "mouse_model.restore(weights_file, head_i=head)\n",
        "mouse_model.build_ensemble(rc, shifts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1o1JCodLPP_"
      },
      "source": [
        "- ðŸŒŸ Next, we need a the function `central_permutation_seqs_gen` to permutate sequences that contains CTCF motif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KPk-Nk4ZTML"
      },
      "outputs": [],
      "source": [
        "from basenji import stream\n",
        "from akita_utils import central_permutation_seqs_gen, ut_dense\n",
        "\n",
        "batch_size=8\n",
        "seq_coords_df = pd.read_csv(CTCT_table, sep=\"\\t\")\n",
        "\n",
        "preds_stream = stream.PredStreamGen(\n",
        "        mouse_model,\n",
        "        central_permutation_seqs_gen(seq_coords_df, fasta_open, chrom_sizes_table),\n",
        "        batch_size,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hfK-ZfRxSpT"
      },
      "source": [
        "- ðŸŒŸ Now, we will see how permutation on sequence will affect genome 3D organization from strong effect to weak effect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QfEhAa_lejL"
      },
      "outputs": [],
      "source": [
        "from akita_utils import central_permutation_seqs_gen, ut_dense\n",
        "def central_permutation_seqs_gen(\n",
        "    seq_coords_df,\n",
        "    genome_open,\n",
        "    chrom_sizes_table,\n",
        "    permutation_window_shift=0,\n",
        "    revcomp=False,\n",
        "    seq_length=1310720,\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates sequences for a set of genomic coordinates, applying central permutations and optionally\n",
        "    operating on reverse complements, with an additional option for shifting the permutation window.\n",
        "\n",
        "    This generator function takes a DataFrame `seq_coords_df` containing genomic coordinates\n",
        "    (chromosome, start, end, strand), a genome file handler `genome_open` to fetch sequences, and\n",
        "    a table of chromosome sizes `chrom_sizes_table`. It yields sequences with central permutations\n",
        "    around the coordinates specified in `seq_coords_df`, considering an optional shift for the\n",
        "    permutation window. If `rc` is True, the reverse complement of these sequences is generated.\n",
        "\n",
        "    Parameters:\n",
        "    - seq_coords_df (pandas.DataFrame): DataFrame with columns 'chrom', 'start', 'end', 'strand',\n",
        "                                        representing genomic coordinates of interest.\n",
        "    - genome_open (GenomeFileHandler): A file handler for the genome to fetch sequences.\n",
        "    - chrom_sizes_table (pandas.DataFrame): DataFrame with columns 'chrom' and 'size', representing\n",
        "                                            the sizes of chromosomes in the genome.\n",
        "    - permutation_window_shift (int, optional): The number of base pairs to shift the center of the\n",
        "                                                 permutation window. Default is 0.\n",
        "    - rc (bool, optional): If True, operates on reverse complement of the sequences. Default is False.\n",
        "    - seq_length (int, optional): The total length of the sequence to be generated. Default is 1310720.\n",
        "\n",
        "    Yields:\n",
        "    numpy.ndarray: One-hot encoded DNA sequences. Each sequence is either the original or its central\n",
        "                   permutation, with or without reverse complement as specified by `rc`.\n",
        "\n",
        "    Raises:\n",
        "    Exception: If the prediction window for a given span cannot be centered within the chromosome.\n",
        "    \"\"\"\n",
        "\n",
        "    for s in seq_coords_df.itertuples():\n",
        "        list_1hot = []\n",
        "\n",
        "        chrom, window_start, window_end = expand_and_check_window(\n",
        "            s, chrom_sizes_table, shift=permutation_window_shift\n",
        "        )\n",
        "        permutation_start, permutation_end = get_relative_window_coordinates(\n",
        "            s, shift=permutation_window_shift\n",
        "        )\n",
        "\n",
        "        wt_seq_1hot = dna_1hot(\n",
        "            genome_open.fetch(chrom, window_start, window_end).upper()\n",
        "        )\n",
        "        if revcomp:\n",
        "            rc_wt_seq_1hot = hot1_rc(wt_seq_1hot)\n",
        "            list_1hot.append(rc_wt_seq_1hot.copy())\n",
        "        else:\n",
        "            list_1hot.append(wt_seq_1hot.copy())\n",
        "\n",
        "        ### MODIFY HERE ###\n",
        "        alt_seq_1hot = wt_seq_1hot.copy()\n",
        "        permuted_span = permute_seq_k(\n",
        "            alt_seq_1hot[permutation_start:permutation_end], k=1\n",
        "        )\n",
        "        alt_seq_1hot[permutation_start:permutation_end] = permuted_span\n",
        "        ### MODIFY HERE ###\n",
        "\n",
        "        if revcomp:\n",
        "            rc_alt_seq_1hot = hot1_rc(alt_seq_1hot.copy())\n",
        "            list_1hot.append(rc_alt_seq_1hot)\n",
        "        else:\n",
        "            list_1hot.append(alt_seq_1hot)\n",
        "\n",
        "        # yielding first the reference, then the permuted sequence\n",
        "        for sequence in list_1hot:\n",
        "            yield sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmnrb1gQZTRH"
      },
      "outputs": [],
      "source": [
        "target_index = 1 # cell type\n",
        "num_experiments = len(seq_coords_df)\n",
        "\n",
        "for ref_index in range(0, num_experiments*2, 2):\n",
        "\n",
        "    ref_preds_matrix = preds_stream[ref_index]\n",
        "    permut_index = ref_index + 1\n",
        "    permuted_preds_matrix = preds_stream[permut_index]\n",
        "    exp_index = ref_index//2\n",
        "\n",
        "    ref_maps = ut_dense(ref_preds_matrix)\n",
        "    perm_maps = ut_dense(permuted_preds_matrix)\n",
        "\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(10, 5))\n",
        "    sns.heatmap(\n",
        "        ref_maps[:,:,target_index],\n",
        "        vmin=-0.6,\n",
        "        vmax=0.6,\n",
        "        cbar=False,\n",
        "        cmap=\"RdBu_r\",\n",
        "        square=True,\n",
        "        xticklabels=False,\n",
        "        yticklabels=False,\n",
        "        ax=axs[0]\n",
        "    )\n",
        "    axs[0].set_title('Reference')\n",
        "\n",
        "    sns.heatmap(\n",
        "        perm_maps[:,:,target_index],\n",
        "        vmin=-0.6,\n",
        "        vmax=0.6,\n",
        "        cbar=False,\n",
        "        cmap=\"RdBu_r\",\n",
        "        square=True,\n",
        "        xticklabels=False,\n",
        "        yticklabels=False,\n",
        "        ax=axs[1]\n",
        "    )\n",
        "    axs[1].set_title('Permuted')\n",
        "\n",
        "    sns.heatmap(\n",
        "        perm_maps[:,:,target_index]-ref_maps[:,:,target_index],\n",
        "        vmin=-0.6,\n",
        "        vmax=0.6,\n",
        "        cbar=True,\n",
        "        cmap=\"PiYG_r\",\n",
        "        square=True,\n",
        "        xticklabels=False,\n",
        "        yticklabels=False,\n",
        "        ax=axs[2]\n",
        "    )\n",
        "    axs[2].set_title('Difference')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3hmtaW7cK6w"
      },
      "source": [
        "## Predictions for inserted CTCF sequences (for mouse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvdUgmZnk3Ud"
      },
      "outputs": [],
      "source": [
        "from akita_utils import dna_1hot, hot1_rc, _insert_casette\n",
        "\n",
        "def symmertic_insertion_seqs_gen(seq_coords_df, background_seqs, genome_open, nproc=1, map=map):\n",
        "    \"\"\"\n",
        "    Generate sequences with symmetric insertions for a given set of coordinates.\n",
        "\n",
        "    This generator function takes a DataFrame `seq_coords_df` containing genomic\n",
        "    coordinates, a list of background sequences `background_seqs`, and a genome file\n",
        "    handler `genome_open`. It yields one-hot encoded DNA sequences with symmetric\n",
        "    insertions based on the specified coordinates.\n",
        "\n",
        "    Parameters:\n",
        "    - seq_coords_df (pandas.DataFrame): DataFrame with columns 'chrom', 'start', 'end',\n",
        "                                         'strand', 'flank_bp', 'spacer_bp', 'orientation'.\n",
        "                                         Represents genomic coordinates and insertion parameters.\n",
        "    - background_seqs (List[numpy.ndarray]): List of background sequences to be modified.\n",
        "    - genome_open (GenomeFileHandler): A file handler for the genome to fetch sequences.\n",
        "\n",
        "    Yields:\n",
        "    numpy.ndarray: One-hot encoded DNA sequence with symmetric insertions.\n",
        "    \"\"\"\n",
        "\n",
        "    for s in seq_coords_df.itertuples():\n",
        "\n",
        "        flank_bp = s.flank_bp\n",
        "        spacer_bp = s.spacer_bp\n",
        "        orientation_string = s.orientation\n",
        "\n",
        "        seq_1hot_insertion = dna_1hot(\n",
        "            genome_open.fetch(\n",
        "                s.chrom, s.start - flank_bp, s.end + flank_bp\n",
        "            ).upper()\n",
        "        )\n",
        "\n",
        "        if s.strand == \"-\":\n",
        "            seq_1hot_insertion = hot1_rc(seq_1hot_insertion)\n",
        "            # now, all motifs are standarized to this orientation \">\"\n",
        "\n",
        "        seq_1hot = background_seqs[s.background_index].copy()\n",
        "\n",
        "        ### MODIFY HERE ###\n",
        "        seq_1hot = _insert_casette(\n",
        "            seq_1hot, seq_1hot_insertion, spacer_bp, orientation_string\n",
        "        )\n",
        "        ### MODIFY HERE ###\n",
        "        # Parameters of _insert_casette:\n",
        "        # - seq_1hot (numpy.ndarray): One-hot encoded DNA sequence to be modified.\n",
        "        # - seq_1hot_insertion (numpy.ndarray): One-hot encoded DNA sequence to be inserted.\n",
        "        # - spacer_bp (int): Number of base pairs for intert-insert spacers.\n",
        "        # - orientation_string (str): String specifying the orientation and number of insertions.\n",
        "        #                         '>' denotes forward orientation, and '<' denotes reverse.\n",
        "\n",
        "        yield seq_1hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3u667KDcP6L"
      },
      "outputs": [],
      "source": [
        "CTCT_table = \"./tutorial_materials/data/insertion_disruption_tsvs/insertion_examples.tsv\"\n",
        "background_file = \"./tutorial_materials/data/background_sequences_model_0.fa\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmLyFPa-XEC7"
      },
      "outputs": [],
      "source": [
        "seq_coords_df = pd.read_csv(CTCT_table, sep=\"\\t\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItFDgeMrXMDL"
      },
      "outputs": [],
      "source": [
        "from akita_utils import dna_1hot\n",
        "\n",
        "background_seqs = []\n",
        "\n",
        "with open(background_file, \"r\") as f:\n",
        "    for line in f.readlines():\n",
        "        if \">\" in line:\n",
        "            continue\n",
        "        background_seqs.append(dna_1hot(line.strip()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1yPbmxsXuww"
      },
      "outputs": [],
      "source": [
        "# predictions for references\n",
        "backgrounds_predictions = mouse_model.predict(np.array(background_seqs), batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmfQYEWRYYTk"
      },
      "outputs": [],
      "source": [
        "from akita_utils import symmertic_insertion_seqs_gen\n",
        "\n",
        "preds_stream = stream.PredStreamGen(\n",
        "        mouse_model,\n",
        "        symmertic_insertion_seqs_gen(seq_coords_df, background_seqs, fasta_open),\n",
        "        batch_size,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhsMFJrjYtzc"
      },
      "outputs": [],
      "source": [
        "target_index = 1 #cell type\n",
        "\n",
        "for exp_index in range(num_experiments):\n",
        "\n",
        "    bg_index = seq_coords_df.iloc[exp_index].background_index\n",
        "\n",
        "    prediction_matrix = preds_stream[exp_index]\n",
        "    reference_prediction_matrix = backgrounds_predictions[bg_index, :, :]\n",
        "\n",
        "    ref_maps = ut_dense(reference_prediction_matrix)\n",
        "    alt_maps = ut_dense(prediction_matrix)\n",
        "\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(10, 4))\n",
        "    sns.heatmap(\n",
        "        ref_maps[:,:,target_index],\n",
        "        vmin=-0.6,\n",
        "        vmax=0.6,\n",
        "        cbar=False,\n",
        "        cmap=\"RdBu_r\",\n",
        "        square=True,\n",
        "        xticklabels=False,\n",
        "        yticklabels=False,\n",
        "        ax=axs[0]\n",
        "    )\n",
        "    axs[0].set_title('Reference')\n",
        "\n",
        "    sns.heatmap(\n",
        "        alt_maps[:,:,target_index],\n",
        "        vmin=-0.6,\n",
        "        vmax=0.6,\n",
        "        cbar=False,\n",
        "        cmap=\"RdBu_r\",\n",
        "        square=True,\n",
        "        xticklabels=False,\n",
        "        yticklabels=False,\n",
        "        ax=axs[1]\n",
        "    )\n",
        "    axs[1].set_title('Inserted')\n",
        "\n",
        "    sns.heatmap(\n",
        "        alt_maps[:,:,target_index]-ref_maps[:,:,target_index],\n",
        "        vmin=-0.6,\n",
        "        vmax=0.6,\n",
        "        cbar=True,\n",
        "        cmap=\"PiYG_r\",\n",
        "        square=True,\n",
        "        xticklabels=False,\n",
        "        yticklabels=False,\n",
        "        ax=axs[2]\n",
        "    )\n",
        "    axs[2].set_title('Difference')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S833owEvjrlU"
      },
      "source": [
        "# Deep Learning in 3D Genome (Training Part Tutorial)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QamAO7-1Hq8"
      },
      "source": [
        "## Understanding the procedure of model training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAS4LsbaB6je"
      },
      "outputs": [],
      "source": [
        "from akita_utils import show_targets, show_prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDBPhKa9nywj"
      },
      "source": [
        "- ðŸŒŸ In the interest of time, we use synthetic data to describe how we go about training an Akita model.\\\n",
        "\\\n",
        "For synthetic data we have freedom to both choose both the size of input sequences as well as the rules used to generate target outputs. Because of this we can design datasets where training is much faster than training on real genomic sequences and experimental Hi-C data by using shorter sequences (here ~32 kb instead of ~1.4 Mb) with simpler features to learn.\\\n",
        "\\\n",
        "More generallh, using synthetic data is a useful strategy for model development, including debugging data preprocessing, or a model's loss function and architecture. \\\n",
        "\\\n",
        "To create this synthetic data, we:\n",
        "- (i) generate random DNA sequences with size of 32768 bp.\n",
        "- (ii) for each sequences we add 4-8 motifs randomly.\n",
        "- (iii) create a corresponding output map by placing boundaries based on where motifs are inserted, which means we create squares between inserted CTCF motifs. \\\n",
        "\\\n",
        "Let's generate the training data with `generate_training_data.py`, which is under `/tutorial_materials/training_materials/`. \\"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwhKcV4pONWS"
      },
      "outputs": [],
      "source": [
        "%cd /content/akita_tutorial\n",
        "!/usr/local/bin/python3 ./tutorial_materials/training_materials/generate_training_data.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjONNgsowRSZ"
      },
      "outputs": [],
      "source": [
        "data_dir = './tutorial_materials/training_materials'\n",
        "\n",
        "# plotting utility to show three synthetic target maps\n",
        "show_targets(data_dir, split_label='train')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngMVpYbdxNOV"
      },
      "source": [
        "- ðŸŒŸ Then, let's take a look at prediction of a naive model (without training). We can tell predictions are far away from targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p75oj5YG1Hq9"
      },
      "outputs": [],
      "source": [
        "model_dir = './tutorial_materials/training_materials/'\n",
        "show_prediction(data_dir, model_dir, split_label='train')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUp09UrKLFyb"
      },
      "source": [
        "- ðŸŒŸ Let's take a look at layers of this model, you will find that combining pooling layers takes us from an input sequence of 32,768 to a set of latent profiles binned at a 64 bp resolution, which is also the resolution of the target maps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBuUH-f2Ksef"
      },
      "outputs": [],
      "source": [
        "params_file = model_dir+'params.json' # architecture\n",
        "with open(params_file, 'r') as file:\n",
        "    params = json.load(file)\n",
        "    model_architecture = params['model']\n",
        "seqnn_model = seqnn.SeqNN(model_architecture)\n",
        "seqnn_model.model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5DJSZH4QSi7"
      },
      "source": [
        "\n",
        "- ðŸŒŸ Before training the model, let's briefly go through the params.json to understand some parameters for training procedure. The params.json is composed of two parts \"train\", which includes parameters for instructing training procedure, and \"model\", which includes parameters for creating model architecture. Here, we will focus on \"train\" part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFHyjONvQXhb"
      },
      "outputs": [],
      "source": [
        "model_dir = './tutorial_materials/training_materials/'\n",
        "params_file = model_dir+'params.json' # architecture\n",
        "with open(params_file, 'r') as file:\n",
        "    params = json.load(file)\n",
        "    train_params = params['train'] # Retrieve model's architecture from params.json\n",
        "\n",
        "train_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C38bhB6K_lh"
      },
      "source": [
        "- ðŸŒŸ By tuning these parameters, we can control the training of a model to achieve better performance and avoid common issues like overfitting and exploding gradients:\n",
        "> 1. **batch_size**:\n",
        "  The batch size defines the number of samples that will be passed into the network at one time. In this case, we use a batch size of 8. A smaller batch size can lead to more noisy estimates of the gradient but can require less memory. For long input sequences as we use for Akita, total GPU memory poses an important limitation on batch size.\n",
        "  2. **optimizer**:\n",
        "  The optimizer used for training the model. In this case, we are using the Adam optimizer, which is an adaptive learning rate optimization algorithm that's been designed specifically for training deep neural networks.\n",
        "  3. **learning_rate**:\n",
        "  Learning rate is a hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated. A higher learning rate can lead to faster convergence but might overshoot the minimum, while a lower learning rate might result in slower convergence.\n",
        "  4. **momentum**:\n",
        "  Momentum is a hyperparameter that controls how heavily to weight previous updates when making the current update, and can lead to faster convergence to better local optima.\n",
        "  5. **loss**:\n",
        "  The loss function used to measure the performance of the model. Here, we are using Mean Squared Error (MSE), which calculates the average of the squares of the errors between predicted and actual values. MSE is commonly used for regression tasks.\n",
        "  6. **patience**:  \n",
        "  Patience is a parameter used in early stopping, which stops the training process if the validation loss for a model does not improve after a certain number of epochs (i.e. full passes through the training data set). Here, the model will stop training if it does not improve for 8 consecutive epochs.\n",
        "  7. **clip_norm**:\n",
        "  Gradient clipping is a technique to prevent exploding gradients in very deep networks. The clip norm parameter specifies the maximum norm for the gradients. If the gradients exceed this norm, they will be scaled down to the maximum norm of 10.0.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da_N7Kx3MXwp"
      },
      "source": [
        "- ðŸŒŸ Now let's run the training script! \\\n",
        "\\\n",
        "This script iterates over all of the `train.tfr` TfRecords of synthetic data contained in a folder in `tutorial_materials/training_materials/`. \\\n",
        "\\\n",
        "Model architecture and training parameters are specified with the `params.json`. Updates to model parameters after each batch are determined based on the loss and the training parameters. The model will stop training after the validation loss saturates (on the `valid.tfr` files) and the patience is exceeded. \\\n",
        "\\\n",
        "Model checkpoints, including the weights, will be stored in the folder `train_out/`. \\\n",
        "\\\n",
        "We use a customized plotting function to visualize the improvement in model performance over time on both the training and validation sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4w55JPv1Hq9"
      },
      "outputs": [],
      "source": [
        "%run bin/akita_train.py -k -o ./train_out/  tutorial_materials/training_materials/params.json tutorial_materials/training_materials/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDixi3vPQnMS"
      },
      "source": [
        "- ðŸŒŸ Finally, let's examine the model's performance after training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjB4zzAzX7wb"
      },
      "outputs": [],
      "source": [
        "show_targets(data_dir, split_label='test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWvU_Dbzzuoj"
      },
      "outputs": [],
      "source": [
        "model_dir = './train_out/'\n",
        "show_prediction(data_dir, model_dir, restore_weights=True, split_label='test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASchEc8hrPTC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
